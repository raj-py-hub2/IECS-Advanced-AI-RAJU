{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='imdb.gin', imports=[], includes=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "from mltrainer import tokenizer, Trainer, metrics\n",
    "from mltrainer.rnn_models import NLPmodel, AttentionNLP\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "import gin\n",
    "gin.parse_config_file(\"imdb.gin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the streamers from the datasetfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-21 14:26:36.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /home/sarmad/.cache/mads_datasets/imdb\u001b[0m\n",
      "\u001b[32m2025-02-21 14:26:36.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /home/sarmad/.cache/mads_datasets/imdb/aclImdb_v1.tar.gz\u001b[0m\n",
      "\u001b[32m2025-02-21 14:26:38.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.factories.basicfactories\u001b[0m:\u001b[36mcreate_dataset\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mCreating TextDatasets from 25000 trainfilesand 25000 testfiles.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 25000/25000 [00:00<00:00, 82048.01it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 25000/25000 [00:00<00:00, 83906.15it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = imdbdatasetfactory.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset_url: https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
       "filename: aclImdb_v1.tar.gz\n",
       "name: imdb\n",
       "unzip: True\n",
       "formats: [<FileTypes.TXT: '.txt'>]\n",
       "digest: 7c2ac02c03563afcf9b574c7e56c153a\n",
       "maxvocab: 10000\n",
       "maxtokens: 100\n",
       "clean_fn: <function clean at 0x716888901800>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbdatasetfactory.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-21 14:26:40.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.tokenizer\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mFound 79808 tokens\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mltrainer.tokenizer import IMDBTokenizer\n",
    "\n",
    "tokenizer = IMDBTokenizer.fromSettings(\n",
    "    traindataset=traindataset,\n",
    "    settings=imdbdatasetfactory.settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-21 14:26:40.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /home/sarmad/.cache/mads_datasets/imdb\u001b[0m\n",
      "\u001b[32m2025-02-21 14:26:40.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /home/sarmad/.cache/mads_datasets/imdb/aclImdb_v1.tar.gz\u001b[0m\n",
      "\u001b[32m2025-02-21 14:26:42.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.factories.basicfactories\u001b[0m:\u001b[36mcreate_dataset\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mCreating TextDatasets from 25000 trainfilesand 25000 testfiles.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 25000/25000 [00:00<00:00, 86730.92it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 25000/25000 [00:00<00:00, 81519.28it/s]\n"
     ]
    }
   ],
   "source": [
    "streamers = imdbdatasetfactory.create_datastreamer(batchsize=32, preprocessor=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 142,  161,  145,  ...,   18,   69,   10],\n",
       "         [ 427, 3414, 7244,  ...,   34,   97,  950],\n",
       "         [1650,    1,  153,  ...,   12,    4,  323],\n",
       "         ...,\n",
       "         [  10,  431,   81,  ...,   27,  138,   84],\n",
       "         [1308,   22,  460,  ...,   22,   41,  177],\n",
       "         [  10,   38,    2,  ...,    0,    0,    0]], dtype=torch.int32),\n",
       " tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 1, 1, 0, 0, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = streamers[\"train\"]\n",
    "batch = train.batchloop()\n",
    "tokenizer(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches 781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = streamers[\"train\"]\n",
    "print(f\"number of batches {len(train)}\")\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = streamers[\"valid\"].stream()\n",
    "X, y = next(iter(trainstreamer))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  10, 1493,   30,  ...,  190,    9,  149],\n",
       "        [  10,   25,   75,  ...,    2, 3175, 1448],\n",
       "        [  93,   93,  139,  ...,    1, 3047,    8],\n",
       "        ...,\n",
       "        [  58, 1741, 6676,  ...,    7, 7828,    6],\n",
       "        [  11,    1,    5,  ...,  217,   42, 1244],\n",
       "        [  72,   89,   25,  ...,   65,  181,    6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset has 782 batches of 32 examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup accuracy and loss_fn (this is a classification problem with two classes, 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "log_dir = Path(\"../../models/nlp/\").resolve()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic config. We need to specify the vocabulary lenght for the embedding layer.\n",
    "Trainsteps are set to just 100 batches for speedup in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-21 14:26:43.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.settings\u001b[0m:\u001b[36mcheck_path\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mCreated logdir /home/sarmad/Documents/code/upperkaam/notebooks_review/Deliverable_Part_3/models/nlp\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: /home/sarmad/Documents/code/upperkaam/notebooks_review/Deliverable_Part_3/models/nlp\n",
       "train_steps: 100\n",
       "valid_steps: 25\n",
       "reporttypes: [<ReportTypes.TENSORBOARD: 2>, <ReportTypes.GIN: 1>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mltrainer import TrainerSettings, ReportTypes\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=100,\n",
    "    valid_steps=25,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.GIN],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gin.get_bindings(\"NLPmodel\")[\"config\"][\"vocab\"] == imdbdatasetfactory.settings.maxvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLPmodel(\n",
       "  (emb): Embedding(10000, 128)\n",
       "  (rnn): GRU(128, 128, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NLPmodel()\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base NLP model is just a GRU, with an embedding as a first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "**Note:** for now we will run with 'cpu', comment the following code line to run according to available device\n",
    "\n",
    "**Code added below**\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment if you want to run it with cuda (gpu)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-21 14:26:43.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to /home/sarmad/Documents/code/upperkaam/notebooks_review/Deliverable_Part_3/models/nlp/20250221-142643\u001b[0m\n",
      "\u001b[32m2025-02-21 14:26:43.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:12<00:00,  8.11it/s]\n",
      "\u001b[32m2025-02-21 14:26:57.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.6926 test 0.6888 metric ['0.5513']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.55it/s]\n",
      "\u001b[32m2025-02-21 14:27:09.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.6889 test 0.6769 metric ['0.5863']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.37it/s]\n",
      "\u001b[32m2025-02-21 14:27:22.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.6881 test 0.6925 metric ['0.4800']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:27:22.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.6769, current loss 0.6925.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.39it/s]\n",
      "\u001b[32m2025-02-21 14:27:35.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.6939 test 0.7124 metric ['0.4900']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:27:35.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.6769, current loss 0.7124.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.48it/s]\n",
      "\u001b[32m2025-02-21 14:27:48.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.6941 test 0.6818 metric ['0.5613']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:27:48.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.6769, current loss 0.6818.Counter 3/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:12<00:00,  8.22it/s]\n",
      "\u001b[32m2025-02-21 14:28:01.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 5 train 0.6876 test 0.6902 metric ['0.5400']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:28:01.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.6769, current loss 0.6902.Counter 4/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:12<00:00,  8.22it/s]\n",
      "\u001b[32m2025-02-21 14:28:15.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 6 train 0.6865 test 0.6985 metric ['0.5637']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:28:15.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.6769, current loss 0.6985.Counter 5/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.42it/s]\n",
      "\u001b[32m2025-02-21 14:28:28.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 7 train 0.6902 test 0.6634 metric ['0.6238']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:10<00:00,  9.63it/s]\n",
      "\u001b[32m2025-02-21 14:28:39.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 8 train 0.6330 test 0.5845 metric ['0.6987']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:09<00:00, 10.20it/s]\n",
      "\u001b[32m2025-02-21 14:28:50.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 9 train 0.5672 test 0.5252 metric ['0.7500']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [02:06<00:00, 12.64s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the impact of attention\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer:** the model with `attension` has faster convergence to minima, better accuracy, and reduced loss due to better long-range dependency capture. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-21 14:28:50.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to /home/sarmad/Documents/code/upperkaam/notebooks_review/Deliverable_Part_3/models/nlp/20250221-142850\u001b[0m\n",
      "\u001b[32m2025-02-21 14:28:50.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.86it/s]\n",
      "\u001b[32m2025-02-21 14:29:02.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.6744 test 0.6748 metric ['0.5575']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.90it/s]\n",
      "\u001b[32m2025-02-21 14:29:14.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.6208 test 0.5989 metric ['0.6625']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:10<00:00,  9.26it/s]\n",
      "\u001b[32m2025-02-21 14:29:26.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.5365 test 0.5282 metric ['0.7362']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.83it/s]\n",
      "\u001b[32m2025-02-21 14:29:38.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.5041 test 0.4922 metric ['0.7700']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:12<00:00,  7.97it/s]\n",
      "\u001b[32m2025-02-21 14:29:52.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.4850 test 0.5004 metric ['0.7412']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:29:52.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.4922, current loss 0.5004.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:12<00:00,  8.29it/s]\n",
      "\u001b[32m2025-02-21 14:30:05.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 5 train 0.4544 test 0.4119 metric ['0.8013']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:13<00:00,  7.48it/s]\n",
      "\u001b[32m2025-02-21 14:30:19.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 6 train 0.4073 test 0.4228 metric ['0.7975']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:30:19.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.4119, current loss 0.4228.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.76it/s]\n",
      "\u001b[32m2025-02-21 14:30:32.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 7 train 0.4175 test 0.4221 metric ['0.7913']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:30:32.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.4119, current loss 0.4221.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.70it/s]\n",
      "\u001b[32m2025-02-21 14:30:44.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 8 train 0.3966 test 0.4470 metric ['0.7950']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:30:44.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.4119, current loss 0.4470.Counter 3/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:11<00:00,  8.77it/s]\n",
      "\u001b[32m2025-02-21 14:30:56.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 9 train 0.3829 test 0.4290 metric ['0.8187']\u001b[0m\n",
      "\u001b[32m2025-02-21 14:30:56.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.4119, current loss 0.4290.Counter 4/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [02:06<00:00, 12.67s/it]\n"
     ]
    }
   ],
   "source": [
    "attentionmodel = AttentionNLP()\n",
    "\n",
    "attentiontrainer = Trainer(\n",
    "    model=attentionmodel,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device=device,\n",
    "    )\n",
    "\n",
    "attentiontrainer.loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
