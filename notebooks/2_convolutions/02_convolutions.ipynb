{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.129'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import mltrainer\n",
    "mltrainer.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 13:51:34.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /home/sarmad/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2025-02-15 13:51:34.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /home/sarmad/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "preprocessor = BasePreprocessor()\n",
    "\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "# flowersfactory = DatasetFactoryProvider.create_factory(DatasetType.FLOWERS)\n",
    "# streamers = flowersfactory.create_datastreamer(batchsize=32, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 156)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain an item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image follows the channels-first convention: (channel, width, height). The label is an integer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pull this through a Conv2d layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    padding=(1,1))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening here? Can you explain all the parameters, and relate them to the outputshape?\n",
    "\n",
    "Let's see what happens if we change the padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 26, 26])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    padding=(0,0))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we change the stride from the default 1 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 14, 14])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    padding=(1,1),\n",
    "    stride=2)\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "After changing out_channels to 128, the output will have 128 channels (filters).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels, \n",
    "    out_channels=128, \n",
    "    kernel_size=3, \n",
    "    padding=(1,1))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "If we change kernel_size to 5, then a larger kernel size (5x5) requires more padding (padding=(2,2)) to maintain the input spatial dimensions.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels, \n",
    "    out_channels=64, \n",
    "    kernel_size=5, \n",
    "    padding=(2,2))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Finally, if no padding is added then output spatial dimensions shrink.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 26, 26])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels, \n",
    "    out_channels=64, \n",
    "    kernel_size=3, \n",
    "    padding=(0,0))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you need to think about what is going in and out of the convolution. We can stitch multiple layers together like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutions = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    ")\n",
    "out = convolutions(x)\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dimensions of the featuremap have become really small. You need to take this into account: If we would have started with a smaller image, we could get errors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\n"
     ]
    }
   ],
   "source": [
    "x_too_small = torch.rand((32, 1, 12, 12))\n",
    "\n",
    "try:\n",
    "    convolutions(x_too_small)\n",
    "except RuntimeError as err:\n",
    "    print(\"ERROR:\", err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our `out` has 32 activation maps, each 2x2 big.\n",
    "\n",
    "If we want to pull the activation maps through a neural network (A dense layer) we will need to flatten them (do you understand what happens if you dont do that?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_nn = nn.Flatten()(out)\n",
    "input_nn.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are potential problems connecting the image layers and the linear layers:\n",
    "- Conv2d and MaxPool both expect 4 dimensional data (batch, channels/activationmaps, width, height)\n",
    "- Linear layers expect 2 dimensional data (batch, features)\n",
    "- Linear layers wont crash if you feed them data with more dimensions! However, they will just work on the last dimension, and thats probably not what you want.\n",
    "\n",
    "This means we need to somehow transform the 4D data into 2D. There are some options here:\n",
    "- Some sort of aggregation; the activationmaps are typically small (eg 2x2) and they indicate that the filter has detected a features. There are a lot of different ways to aggregate this: mean, max, min, sum, etc...\n",
    "- Flatten: a flatten layer simple transforms (batch, C, W, H) into (batch, C * W * H). lets say you have (32, 32, 2, 2) than after a flatten you end up with (32, 128). The problem here is, when you use a different amount of Conv2d layers, or a different stride or padding, you will end up with a different size of activationmap, eg (32, 32, 3, 3), which would mean you would end up with 32 * 3 * 3 = 288 features. \n",
    "\n",
    "I have solved this problem by calculating the size of the activationmap with the ._conv_test method. After I calculate the size of the map (eg (2,2)) I can create an AvgPool2d layer that will take the average of the (2,2) map. This way you will always end up with (batch, filters, 1, 1) and after the flatten this will be filter * 1 * 1, which is exactly the amount of filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgpool = nn.AvgPool2d((2,2))\n",
    "pooled = avgpool(out)\n",
    "pooled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we flatten this, we obtain 32x1x1 numbers, which is still 32, which makes designing your model a bit easier (and you might also argue that taking the average is a good approach in terms of model logic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine it all together, and add a _conv_test method to create the right size for the AvgPool2D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from loguru import logger\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "\n",
    "\n",
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, filters: int, units1: int, units2: int, input_size: tuple):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_size[1]\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        activation_map_size = self._conv_test(self.input_size)\n",
    "        logger.info(f\"Aggregating activationmap with size {activation_map_size}\")\n",
    "        self.agg = nn.AvgPool2d(activation_map_size)\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(filters, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, 10)\n",
    "        )\n",
    "\n",
    "    def _conv_test(self, input_size):\n",
    "        x = torch.ones(input_size, dtype=torch.float32)\n",
    "        x = self.convolutions(x)\n",
    "        return x.shape[-2:]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.agg(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 13:51:35.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([26, 26])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 224, 224]           3,584\n",
      "              ReLU-2        [-1, 128, 224, 224]               0\n",
      "         MaxPool2d-3        [-1, 128, 112, 112]               0\n",
      "            Conv2d-4        [-1, 128, 110, 110]         147,584\n",
      "              ReLU-5        [-1, 128, 110, 110]               0\n",
      "         MaxPool2d-6          [-1, 128, 55, 55]               0\n",
      "            Conv2d-7          [-1, 128, 53, 53]         147,584\n",
      "              ReLU-8          [-1, 128, 53, 53]               0\n",
      "         MaxPool2d-9          [-1, 128, 26, 26]               0\n",
      "        AvgPool2d-10            [-1, 128, 1, 1]               0\n",
      "          Flatten-11                  [-1, 128]               0\n",
      "           Linear-12                  [-1, 128]          16,512\n",
      "             ReLU-13                  [-1, 128]               0\n",
      "           Linear-14                   [-1, 64]           8,256\n",
      "             ReLU-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 324,170\n",
      "Trainable params: 324,170\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 142.99\n",
      "Params size (MB): 1.24\n",
      "Estimated Total Size (MB): 144.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CNN(filters=128, units1=128, units2=64, input_size=(32, 3, 224, 224))\n",
    "summary(model, input_size=(3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 13:51:36.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([2, 2])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 28, 28]           1,280\n",
      "              ReLU-2          [-1, 128, 28, 28]               0\n",
      "         MaxPool2d-3          [-1, 128, 14, 14]               0\n",
      "            Conv2d-4          [-1, 128, 12, 12]         147,584\n",
      "              ReLU-5          [-1, 128, 12, 12]               0\n",
      "         MaxPool2d-6            [-1, 128, 6, 6]               0\n",
      "            Conv2d-7            [-1, 128, 4, 4]         147,584\n",
      "              ReLU-8            [-1, 128, 4, 4]               0\n",
      "         MaxPool2d-9            [-1, 128, 2, 2]               0\n",
      "        AvgPool2d-10            [-1, 128, 1, 1]               0\n",
      "          Flatten-11                  [-1, 128]               0\n",
      "           Linear-12                  [-1, 128]          16,512\n",
      "             ReLU-13                  [-1, 128]               0\n",
      "           Linear-14                   [-1, 64]           8,256\n",
      "             ReLU-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 321,866\n",
      "Trainable params: 321,866\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.08\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 3.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CNN(filters=128, units1=128, units2=64, input_size=(32, 1, 28, 28))\n",
    "summary(model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 15k parameters. You will always need to judge that relative to your input data: \n",
    "\n",
    "- how many observations do you have? \n",
    "- maybe even more important: how many features do you have? Images sized 28x28 will need much less complexity than images sized 224x224 (note how the first one has 784 features, the second one more than 50.000!)\n",
    "- Do you think the model needs a lot of complexity, or not so much? E.g. classifying if there is a stamp, or not, on a piece of paper is much easier than classifying the age of a face.\n",
    "\n",
    "Also think about:\n",
    "What is the trade off between adding more complexity? Or reducing complexity?\n",
    "\n",
    "Try to answer this trade of in terms of:\n",
    "\n",
    "- speed\n",
    "- generalization\n",
    "- accuracy\n",
    "\n",
    "Eg 512 filters might add 0.1 % accuracy, but it might double training time. Is that worth it? Often, not..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "When adjusting model complexity (e.g., increasing filters, units, or layers), we must balance speed, generalization, and accuracy.\n",
    "\n",
    "\n",
    "1. Speed\n",
    "- More parameters increase training time and memory usage.\n",
    "- Larger models might be slower during inference, especially on edge devices.\n",
    "- Example: Increasing filters from 128 to 512 might double the training time but yield very less accuracy gain.\n",
    "\n",
    "2. Generalization\n",
    "- A simple model (fewer parameters) is less likely to overfit but may underfit.\n",
    "- A complex model (many parameters) might memorize the training data instead of learning general patterns.\n",
    "- More parameters require more data to avoid overfitting.\n",
    "\n",
    "3. Accuracy\n",
    "- More complexity = more learning capacity but diminishing returns after a point.\n",
    "- Example: Classifying a stamp on paper is easy → fewer parameters work well.\n",
    "- Example: Predicting age from a face is hard → requires more complexity.\n",
    "\n",
    "Final takeway: A good model is not the biggest one, but the most efficient one for your data!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to tell the model how good it is performing. To do that, we will need to pick a loss function $\\mathcal{L}$. We will discuss this in more depth, but for now, just take my word for it that a CrossEntropyLoss is a good pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from mltrainer import metrics, Trainer\n",
    "optimizer = optim.Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 13:51:36.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([2, 2])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = CNN(filters=128, units1=128, units2=64, input_size=(32, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "accuracy(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../../models/cnn\").resolve()\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: /mlcs/projects/upperkaam/notebooks_review/Deliverable_Part_1/models/cnn\n",
       "train_steps: 937\n",
       "valid_steps: 156\n",
       "reporttypes: [<ReportTypes.TENSORBOARD: 2>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.1, 'patience': 10}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mltrainer import TrainerSettings, ReportTypes\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 13:51:36.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to /mlcs/projects/upperkaam/notebooks_review/Deliverable_Part_1/models/cnn/20250215-135136\u001b[0m\n",
      "\u001b[32m2025-02-15 13:51:37.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 145.08it/s]\n",
      "\u001b[32m2025-02-15 13:51:44.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.7733 test 0.5419 metric ['0.7987']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 158.35it/s]\n",
      "\u001b[32m2025-02-15 13:51:50.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.4534 test 0.4041 metric ['0.8562']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 159.88it/s]\n",
      "\u001b[32m2025-02-15 13:51:57.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3565 test 0.3447 metric ['0.8736']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 172.74it/s]\n",
      "\u001b[32m2025-02-15 13:52:03.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3063 test 0.3176 metric ['0.8861']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 173.89it/s]\n",
      "\u001b[32m2025-02-15 13:52:08.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.2746 test 0.2844 metric ['0.9008']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 174.06it/s]\n",
      "\u001b[32m2025-02-15 13:52:14.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 5 train 0.2543 test 0.3179 metric ['0.8805']\u001b[0m\n",
      "\u001b[32m2025-02-15 13:52:14.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.2844, current loss 0.3179.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 175.99it/s]\n",
      "\u001b[32m2025-02-15 13:52:20.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 6 train 0.2321 test 0.2572 metric ['0.9058']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 175.88it/s]\n",
      "\u001b[32m2025-02-15 13:52:26.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 7 train 0.2132 test 0.2667 metric ['0.9036']\u001b[0m\n",
      "\u001b[32m2025-02-15 13:52:26.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.2572, current loss 0.2667.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 176.37it/s]\n",
      "\u001b[32m2025-02-15 13:52:32.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 8 train 0.2003 test 0.2508 metric ['0.9118']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 168.02it/s]\n",
      "\u001b[32m2025-02-15 13:52:38.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 9 train 0.1867 test 0.2376 metric ['0.9177']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [01:00<00:00,  6.05s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
