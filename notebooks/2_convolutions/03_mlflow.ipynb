{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the mads_datasets package (see [github](https://github.com/raoulg/mads_datasets) for more details) which I created for these lessons to give everyone easy access to the datasets we use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetType.FLOWERS\n",
      "DatasetType.IMDB\n",
      "DatasetType.GESTURES\n",
      "DatasetType.FASHION\n",
      "DatasetType.SUNSPOTS\n",
      "DatasetType.IRIS\n",
      "DatasetType.PENGUINS\n",
      "DatasetType.FAVORITA\n",
      "DatasetType.SECURE\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "\n",
    "for dataset in DatasetType:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few datasets. For images, we can use FLOWERS (~3000 photos of flowers in 5 categories) and FASHION (60k fashion icons 28x28 pixels big)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with our good'ol MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 14:30:06.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /home/sarmad/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:06.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /home/sarmad/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "batchsize = 64\n",
    "preprocessor = BasePreprocessor()\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=batchsize, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain an item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image follows the channels-first convention: (channel, width, height). The label is an integer.\n",
    "\n",
    "Let's re-use the model we had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 14:30:06.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([2, 2])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, filters, units1, units2, input_size=(32, 1, 28, 28)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        activation_map_size = self._conv_test(input_size)\n",
    "        logger.info(f\"Aggregating activationmap with size {activation_map_size}\")\n",
    "        self.agg = nn.AvgPool2d(activation_map_size)\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(filters, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, 10)\n",
    "        )\n",
    "\n",
    "    def _conv_test(self, input_size = (32, 1, 28, 28)):\n",
    "        x = torch.ones(input_size)\n",
    "        x = self.convolutions(x)\n",
    "        return x.shape[-2:]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.agg(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "model = CNN(filters=32, units1=128, units2=64).to(device)   # Change this to make it work with current device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "              ReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 12, 12]           9,248\n",
      "              ReLU-5           [-1, 32, 12, 12]               0\n",
      "         MaxPool2d-6             [-1, 32, 6, 6]               0\n",
      "            Conv2d-7             [-1, 32, 4, 4]           9,248\n",
      "              ReLU-8             [-1, 32, 4, 4]               0\n",
      "         MaxPool2d-9             [-1, 32, 2, 2]               0\n",
      "        AvgPool2d-10             [-1, 32, 1, 1]               0\n",
      "          Flatten-11                   [-1, 32]               0\n",
      "           Linear-12                  [-1, 128]           4,224\n",
      "             ReLU-13                  [-1, 128]               0\n",
      "           Linear-14                   [-1, 64]           8,256\n",
      "             ReLU-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 31,946\n",
      "Trainable params: 31,946\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.52\n",
      "Params size (MB): 0.12\n",
      "Estimated Total Size (MB): 0.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28), device=device) # Change this to make it work with current device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set up the optimizer, loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from mltrainer import metrics\n",
    "optimizer = optim.Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "**Code modified**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1562, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x.to(device))       # Change this to make it work with current device\n",
    "accuracy(y.to(device), yhat)     # Change this to make it work with current device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow\n",
    "MLflow is an open-source platform designed to manage the entire Machine Learning (ML) lifecycle, including experimentation, reproducibility, deployment, and governance. It provides a set of APIs and tools to streamline ML workflows, making it easier to track experiments, package code, manage model versions, and deploy models.\n",
    "\n",
    "Reasons to use MLflow over TensorBoard, gin-config, or Ray:\n",
    "\n",
    "- End-to-end ML lifecycle management: While TensorBoard focuses on visualizing model training metrics and gin-config on hyperparameter configuration, MLflow covers a broader range of tasks, such as experiment tracking, model packaging, and deployment.\n",
    "\n",
    "- Framework agnostic: MLflow is not tied to a specific ML framework, making it suitable for projects using different libraries or even multiple libraries.\n",
    "\n",
    "- Model Registry: MLflow provides a centralized model registry, allowing you to version, track, and manage your models, which is not available in TensorBoard or gin-config.\n",
    "\n",
    "- Deployment support: MLflow facilitates model deployment to various platforms, such as local, cloud, or Kubernetes environments, whereas TensorBoard and gin-config are not built for deployment tasks.\n",
    "\n",
    "- Integration with other tools: MLflow integrates with popular tools and platforms like Databricks, AWS, and Azure, making it easy to incorporate into existing workflows.\n",
    "\n",
    "However, the choice between MLflow and other tools like TensorBoard, gin-config, or Ray depends on your specific use case and the scope of the ML workflow you want to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = \"mlflow_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/15 14:30:07 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/02/15 14:30:07 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/02/15 14:30:07 INFO mlflow.tracking.fluent: Experiment with name 'mlflow_test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/sarmad/Documents/code/upperkaam/notebooks_review/Deliverable_Part_1/notebooks/2_convolutions/mlruns/1', creation_time=1739597407408, experiment_id='1', last_update_time=1739597407408, lifecycle_stage='active', name='mlflow_test', tags={}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we set the MLflow tracking URI to a local SQLite database file. This is done to configure the storage location for MLflow's experiment tracking data, such as metrics, parameters, and artifacts. By specifying a SQLite database, we enable a lightweight and easy-to-use storage solution for tracking the experiments and their associated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import functions and classes from the hyperopt library to perform hyperparameter optimization. This library helps us find the best hyperparameter values for our machine learning model by searching through a defined search space and using optimization algorithms like Tree-structured Parzen Estimator (TPE). The goal is to improve our model's performance by tuning its hyperparameters.\n",
    "\n",
    "Advantages of TPE:\n",
    "\n",
    "- Model-based approach: TPE is a Bayesian optimization method that models the objective function as a probability distribution. It learns from previous evaluations to decide which points in the search space to explore next, making it more efficient in finding optimal hyperparameters.\n",
    "\n",
    "- Exploration-exploitation trade-off: TPE balances the trade-off between exploration (searching in new regions of the search space) and exploitation (refining around the current best points). This can lead to better results in problems with complex search spaces.\n",
    "\n",
    "- Continuous hyperparameter optimization: TPE can handle continuous hyperparameters more naturally, as it builds a probability model to estimate the performance for any given point in the search space.\n",
    "\n",
    "Lets set up an objective function and start logging some usefull things we might want to track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = Path(\"../../models/mnist\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir()\n",
    "    print(f\"Created {modeldir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 14:30:07.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.settings\u001b[0m:\u001b[36mcheck_path\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mCreated logdir /home/sarmad/Documents/code/upperkaam/notebooks_review/Deliverable_Part_1/notebooks/2_convolutions/modellogs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from mltrainer import metrics, Trainer, TrainerSettings, ReportTypes\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.MLFLOW],\n",
    ")\n",
    "\n",
    "\n",
    "# Define the objective function for hyperparameter optimization\n",
    "def objective(params):\n",
    "    # Start a new MLflow run for tracking the experiment\n",
    "    with mlflow.start_run():\n",
    "        # Set MLflow tags to record metadata about the model and developer\n",
    "        mlflow.set_tag(\"model\", \"convnet\")\n",
    "        mlflow.set_tag(\"dev\", \"raoul\")\n",
    "        # Log hyperparameters to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"batchsize\", f\"{batchsize}\")\n",
    "\n",
    "\n",
    "        # Initialize the optimizer, loss function, and accuracy metric\n",
    "        optimizer = optim.Adam\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        accuracy = metrics.Accuracy()\n",
    "\n",
    "        # Instantiate the CNN model with the given hyperparameters\n",
    "        model = CNN(**params)\n",
    "        # Train the model using a custom train loop\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            device=device,\n",
    "        )\n",
    "        trainer.loop()\n",
    "\n",
    "        # Save the trained model with a timestamp\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        modelpath = modeldir / (tag + \"model.pt\")\n",
    "        torch.save(model, modelpath)\n",
    "\n",
    "        # Log the saved model as an artifact in MLflow\n",
    "        mlflow.log_artifact(local_path=modelpath, artifact_path=\"pytorch_models\")\n",
    "        return {'loss' : trainer.test_loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'filters' : scope.int(hp.quniform('filters', 16, 128, 8)),\n",
    "    'units1' : scope.int(hp.quniform('units1', 32, 128, 8)),\n",
    "    'units2' : scope.int(hp.quniform('units2', 32, 128, 8)),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a search space for hyperparameter optimization using Hyperopt. The search space specifies the range and distribution of hyperparameters to explore during the optimization process. This is crucial for finding the optimal set of hyperparameters that yield the best performance for the machine learning model. The search space defined here includes the number of filters in the convolutional layers, and the number of units in two fully connected layers, allowing Hyperopt to find the best combination within the given ranges.\n",
    "\n",
    "\n",
    "Now, finally, let us perform the hyperparameter search using the fmin function from hyperopt. The function takes the following arguments:\n",
    "\n",
    "- `fn=objective`: The objective function to minimize, which is defined earlier to train the model and return the test loss.\n",
    "- `space=search_space`: The search space defined earlier, containing the range of hyperparameters to explore.\n",
    "- `algo=tpe.suggest`: The optimization algorithm to use, in this case, the Tree-structured Parzen Estimator (TPE) method.\n",
    "- `max_evals=10`: The maximum number of function evaluations, i.e., the maximum number of hyperparameter combinations to try.\n",
    "- `trials=Trials()`: A Trials object to store the results of each evaluation.\n",
    "\n",
    "The fmin function searches for the best hyperparameters within the given search space using the TPE algorithm, aiming to minimize the objective function (test loss). Once the optimization process is completed, the best hyperparameters found are stored in the best_result variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 14:30:07.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([2, 2])\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:07.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to modellogs/20250215-143007\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:08.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/3 [00:00<?, ?it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|\u001b[38;2;30;71;6m8         \u001b[0m| 8/100 [00:00<00:01, 77.22it/s]\u001b[A\n",
      " 35%|\u001b[38;2;30;71;6m###5      \u001b[0m| 35/100 [00:00<00:00, 183.91it/s]\u001b[A\n",
      " 63%|\u001b[38;2;30;71;6m######3   \u001b[0m| 63/100 [00:00<00:00, 223.41it/s]\u001b[A\n",
      " 91%|\u001b[38;2;30;71;6m#########1\u001b[0m| 91/100 [00:00<00:00, 243.93it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 223.29it/s]\n",
      "\u001b[32m2025-02-15 14:30:09.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 1.6577 test 1.0521 metric ['0.5817']\u001b[0m\n",
      " 33%|\u001b[38;2;30;71;6m###3      \u001b[0m| 1/3 [00:00<00:01,  1.49it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 26%|\u001b[38;2;30;71;6m##6       \u001b[0m| 26/100 [00:00<00:00, 254.46it/s]\u001b[A\n",
      " 54%|\u001b[38;2;30;71;6m#####4    \u001b[0m| 54/100 [00:00<00:00, 266.03it/s]\u001b[A\n",
      " 82%|\u001b[38;2;30;71;6m########2 \u001b[0m| 82/100 [00:00<00:00, 270.34it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 270.32it/s]\n",
      "\u001b[32m2025-02-15 14:30:09.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.9397 test 0.8602 metric ['0.6739']\u001b[0m\n",
      " 67%|\u001b[38;2;30;71;6m######6   \u001b[0m| 2/3 [00:01<00:00,  1.59it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|\u001b[38;2;30;71;6m##7       \u001b[0m| 27/100 [00:00<00:00, 262.41it/s]\u001b[A\n",
      " 54%|\u001b[38;2;30;71;6m#####4    \u001b[0m| 54/100 [00:00<00:00, 264.42it/s]\u001b[A\n",
      " 81%|\u001b[38;2;30;71;6m########1 \u001b[0m| 81/100 [00:00<00:00, 265.68it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 259.59it/s]\n",
      "\u001b[32m2025-02-15 14:30:10.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.8022 test 0.7480 metric ['0.7075']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 3/3 [00:01<00:00,  1.62it/s]\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 3/3 [00:01<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:02<00:05,  2.68s/trial, best loss: 0.7480487823486328]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 14:30:10.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([2, 2])\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:10.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to modellogs/20250215-143010\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:10.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/3 [00:00<?, ?it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|\u001b[38;2;30;71;6m#8        \u001b[0m| 18/100 [00:00<00:00, 175.63it/s]\u001b[A\n",
      " 38%|\u001b[38;2;30;71;6m###8      \u001b[0m| 38/100 [00:00<00:00, 184.08it/s]\u001b[A\n",
      " 58%|\u001b[38;2;30;71;6m#####8    \u001b[0m| 58/100 [00:00<00:00, 188.41it/s]\u001b[A\n",
      " 78%|\u001b[38;2;30;71;6m#######8  \u001b[0m| 78/100 [00:00<00:00, 190.15it/s]\u001b[A\n",
      " 98%|\u001b[38;2;30;71;6m#########8\u001b[0m| 98/100 [00:00<00:00, 189.76it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 187.85it/s]\n",
      "\u001b[32m2025-02-15 14:30:11.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 1.5409 test 0.9695 metric ['0.6202']\u001b[0m\n",
      " 33%|\u001b[38;2;30;71;6m###3      \u001b[0m| 1/3 [00:00<00:01,  1.23it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 19%|\u001b[38;2;30;71;6m#9        \u001b[0m| 19/100 [00:00<00:00, 182.83it/s]\u001b[A\n",
      " 38%|\u001b[38;2;30;71;6m###8      \u001b[0m| 38/100 [00:00<00:00, 184.41it/s]\u001b[A\n",
      " 57%|\u001b[38;2;30;71;6m#####6    \u001b[0m| 57/100 [00:00<00:00, 185.68it/s]\u001b[A\n",
      " 77%|\u001b[38;2;30;71;6m#######7  \u001b[0m| 77/100 [00:00<00:00, 188.05it/s]\u001b[A\n",
      " 96%|\u001b[38;2;30;71;6m#########6\u001b[0m| 96/100 [00:00<00:00, 187.71it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 186.95it/s]\n",
      "\u001b[32m2025-02-15 14:30:12.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.8810 test 0.8193 metric ['0.7025']\u001b[0m\n",
      " 67%|\u001b[38;2;30;71;6m######6   \u001b[0m| 2/3 [00:01<00:00,  1.22it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|\u001b[38;2;30;71;6m##        \u001b[0m| 20/100 [00:00<00:00, 196.57it/s]\u001b[A\n",
      " 40%|\u001b[38;2;30;71;6m####      \u001b[0m| 40/100 [00:00<00:00, 192.86it/s]\u001b[A\n",
      " 60%|\u001b[38;2;30;71;6m######    \u001b[0m| 60/100 [00:00<00:00, 188.45it/s]\u001b[A\n",
      " 80%|\u001b[38;2;30;71;6m########  \u001b[0m| 80/100 [00:00<00:00, 191.06it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 190.40it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 190.54it/s]\n",
      "\u001b[32m2025-02-15 14:30:12.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.7601 test 0.7390 metric ['0.7248']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 3/3 [00:02<00:00,  1.23it/s]\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 3/3 [00:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:05<00:02,  2.57s/trial, best loss: 0.7389641404151917]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 14:30:12.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mAggregating activationmap with size torch.Size([2, 2])\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:12.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to modellogs/20250215-143012\u001b[0m\n",
      "\u001b[32m2025-02-15 14:30:12.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/3 [00:00<?, ?it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|\u001b[38;2;30;71;6m##2       \u001b[0m| 22/100 [00:00<00:00, 215.29it/s]\u001b[A\n",
      " 46%|\u001b[38;2;30;71;6m####6     \u001b[0m| 46/100 [00:00<00:00, 224.43it/s]\u001b[A\n",
      " 69%|\u001b[38;2;30;71;6m######9   \u001b[0m| 69/100 [00:00<00:00, 216.32it/s]\u001b[A\n",
      " 93%|\u001b[38;2;30;71;6m#########3\u001b[0m| 93/100 [00:00<00:00, 221.78it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 221.00it/s]\n",
      "\u001b[32m2025-02-15 14:30:13.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 1.5910 test 1.0529 metric ['0.5750']\u001b[0m\n",
      " 33%|\u001b[38;2;30;71;6m###3      \u001b[0m| 1/3 [00:00<00:01,  1.44it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|\u001b[38;2;30;71;6m##2       \u001b[0m| 22/100 [00:00<00:00, 219.30it/s]\u001b[A\n",
      " 46%|\u001b[38;2;30;71;6m####6     \u001b[0m| 46/100 [00:00<00:00, 227.01it/s]\u001b[A\n",
      " 70%|\u001b[38;2;30;71;6m#######   \u001b[0m| 70/100 [00:00<00:00, 229.30it/s]\u001b[A\n",
      " 93%|\u001b[38;2;30;71;6m#########3\u001b[0m| 93/100 [00:00<00:00, 224.96it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 224.29it/s]\n",
      "\u001b[32m2025-02-15 14:30:14.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.8770 test 0.8398 metric ['0.6909']\u001b[0m\n",
      " 67%|\u001b[38;2;30;71;6m######6   \u001b[0m| 2/3 [00:01<00:00,  1.47it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|\u001b[38;2;30;71;6m##5       \u001b[0m| 25/100 [00:00<00:00, 239.80it/s]\u001b[A\n",
      " 49%|\u001b[38;2;30;71;6m####9     \u001b[0m| 49/100 [00:00<00:00, 235.07it/s]\u001b[A\n",
      " 73%|\u001b[38;2;30;71;6m#######3  \u001b[0m| 73/100 [00:00<00:00, 235.02it/s]\u001b[A\n",
      " 97%|\u001b[38;2;30;71;6m#########7\u001b[0m| 97/100 [00:00<00:00, 232.45it/s]\u001b[A\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 100/100 [00:00<00:00, 232.34it/s]\n",
      "\u001b[32m2025-02-15 14:30:15.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.7805 test 0.7684 metric ['0.6970']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 3/3 [00:02<00:00,  1.47it/s]\n",
      "100%|\u001b[38;2;30;71;6m##########\u001b[0m| 3/3 [00:02<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.42s/trial, best loss: 0.7389641404151917]\n"
     ]
    }
   ],
   "source": [
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=3,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this, you can look at the best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filters': np.float64(120.0),\n",
       " 'units1': np.float64(56.0),\n",
       " 'units2': np.float64(80.0)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can also explore the UI from mlflow. It is pretty nice. The help you out, you can use the makefile by first navigating to `/notebooks/2_convolutions` in the terminal and then typing `make show_logs`. This starts a server you can open at `localhost:5000` . Also, have a look at the `Makefile` in this folder to see what you execute. It save the user from typing an inconvenient long and complex command every time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
