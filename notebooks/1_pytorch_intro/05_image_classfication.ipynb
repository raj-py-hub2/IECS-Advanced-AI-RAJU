{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.14'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import mads_datasets\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "\n",
    "mads_datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 12:17:54.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /home/sarmad/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2025-02-15 12:17:54.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /home/sarmad/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datasets = fashionfactory.create_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `Dataset`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTDataset (len 60000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the `__getitem__` method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, torch.Tensor, tensor(1.), tensor(0.), torch.float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = datasets[\"train\"][0]\n",
    "type(x), type(x[0]), type(x[1]), x[0].max(), x[0].min(), x[0].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this (but no one does that, obviously. We implement the dunder method to make life easier, not more complex...):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasets[\"train\"].__getitem__(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can either use pytorches DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(datasets[\"train\"], batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(datasets[\"valid\"], batch_size=64, shuffle=True)\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.76 ms ± 55.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X, y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "preprocessor = BasePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 12:17:58.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /home/sarmad/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2025-02-15 12:17:58.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /home/sarmad/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# or the BaseDatastreamer from the datasetfactory. Check out which one is faster\n",
    "\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor )\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(trainstreamer))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18 ms ± 47.9 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X, y = next(iter(trainstreamer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 156)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(trainstreamer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[1]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x77975d0506d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "**Code modified**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 12:18:08.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mUsing cuda:0 device\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 256]         131,328\n",
      "              ReLU-5                  [-1, 256]               0\n",
      "            Linear-6                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.04\n",
      "Estimated Total Size (MB): 2.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from loguru import logger\n",
    "\n",
    "logger.info(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)      # changed this to make sure we run on same device\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(trainstreamer))\n",
    "model.to(device)\n",
    "next(model.parameters()).is_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(X.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3140, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(yhat, y.to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mlcs/projects/upperkaam/notebooks_review/Deliverable_Part_1/notebooks/1_pytorch_intro/logs')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir=Path(\"logs\").absolute()\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 3\n",
       "metrics: [Accuracy]\n",
       "logdir: /mlcs/projects/upperkaam/notebooks_review/Deliverable_Part_1/notebooks/1_pytorch_intro/logs\n",
       "train_steps: 937\n",
       "valid_steps: 156\n",
       "reporttypes: [<ReportTypes.TENSORBOARD: 'TENSORBOARD'>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.1, 'patience': 10}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mltrainer import TrainerSettings, ReportTypes, Trainer\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 12:18:09.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to /mlcs/projects/upperkaam/notebooks_review/Deliverable_Part_1/notebooks/1_pytorch_intro/logs/20250215-121809\u001b[0m\n",
      "\u001b[32m2025-02-15 12:18:09.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:02<00:00, 392.84it/s]\n",
      "\u001b[32m2025-02-15 12:18:12.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEpoch 0 train 0.4901 test 0.4239 metric ['0.8517']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:02<00:00, 422.47it/s]\n",
      "\u001b[32m2025-02-15 12:18:14.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEpoch 2 train 0.3635 test 0.4063 metric ['0.8577']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:02<00:00, 418.08it/s]\n",
      "\u001b[32m2025-02-15 12:18:17.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEpoch 4 train 0.3211 test 0.3641 metric ['0.8670']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:07<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have the latest model at trainer.model, or just use the old model (which is the same)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a look at the settings.earlystop_kwargs, you can see that save is by default false. If you change this to true, the trainer would have kept track of the best model so far and saved it in between. Because this can take up additional time and in a learning setting like we are in we typically dont really want to save the model for later use, we dont need it here.\n",
    "\n",
    "However, in a real life setting you probably want the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.earlystop_kwargs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeldir exists: True\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\").resolve()\n",
    "print(f'modeldir exists: {model_dir.exists()}')\n",
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case you would have set the earlystop.save to true like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    "    earlystop_kwargs={'save': True, 'verbose': True, 'patience': 10}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer would have saved checkpoints of the last best model. You can obtain the location of the checkpoint with `trainer.early_stopping.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mlcs/projects/upperkaam/notebooks_review/Deliverable_Part_1/notebooks/1_pytorch_intro/logs/20250215-121809/checkpoint.pt')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.early_stopping.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "# note that I would expect the loaded model to run on mps, but that doesnt work as expected\n",
    "if device == \"mps\":\n",
    "    device = \"cpu\"\n",
    "print(f\"using {device}\")\n",
    "loaded_model = torch.load(modelpath, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "for param in loaded_model.parameters():\n",
    "    print(param.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = loaded_model(X.to(device))\n",
    "loss_fn(yhat, y.to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the batch 1 is 92.1875\n"
     ]
    }
   ],
   "source": [
    "acc = (yhat.argmax(dim=1) == y.to(device)).sum() / len(y.to(device))\n",
    "print(f\"The accuracy for the batch 1 is {acc.item() * 100}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "**Now, get the accuracy for few more batchs.**\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the batch 2 is 92.1875\n",
      "The accuracy for the batch 3 is 81.25\n",
      "The accuracy for the batch 4 is 85.9375\n",
      "The accuracy for the batch 5 is 93.75\n",
      "The accuracy for the batch 6 is 84.375\n",
      "The accuracy for the batch 7 is 89.0625\n",
      "The accuracy for the batch 8 is 85.9375\n",
      "The accuracy for the batch 9 is 90.625\n",
      "The accuracy for the batch 10 is 85.9375\n",
      "The accuracy for the batch 11 is 87.5\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    X, y = next(iter(testloader))\n",
    "    yhat = loaded_model(X.to(device))\n",
    "    loss_fn(yhat, y.to(device))\n",
    "    acc = (yhat.argmax(dim=1) == y.to(device)).sum() / len(y.to(device))\n",
    "    print(f\"The accuracy for the batch {i+2} is {acc.item() * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
